from utils import getPrompt , getModel

if __name__ == "__main__":
    llm = getModel()

    user_query = input("ğŸ’¬ User: ")
    ticket_id = input("ğŸ”¢ Ticket ID to check: ")

    try:
        ticket_id = int(ticket_id)
        prompt = getPrompt(user_query, ticket_id)
        response = llm(prompt, max_tokens=1000)
        print("ğŸ¤– Bot:", response["choices"][0]["text"].strip())
    except Exception as e:
        print(f"âŒ LLM failed to generate response: {e}")
